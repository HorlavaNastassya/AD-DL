{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py train /u/horlavanasta/MasterProject//DataAndExperiments/Experiments/Experiments-1.5T-3T/NNs_Bayesian/ResNet18/subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue --n_splits 1 --split 0  --batch_size 5               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fpg(data_batch, indices=None,plane=\"sag\", num_rows=2, \n",
    "    num_cols=2, name=None, folder=\"/current_augmentations_examples/\"):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=((int(8 * num_rows), int(6 * num_cols))))\n",
    "    print(data_batch.shape)\n",
    "    data_batch=data_batch[:num_rows*num_cols].reshape(num_rows, num_cols, data_batch.shape[1],  data_batch.shape[2],  data_batch.shape[3], \n",
    "                                  data_batch.shape[4])\n",
    "    print(data_batch.shape)\n",
    "\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "\n",
    "            i, j, k = indices\n",
    "            data=data_batch[row][col]\n",
    "            kwargs = dict(cmap='gray', interpolation='none')\n",
    "            slices=dict()\n",
    "            slices[\"sag\"], slices[\"cor\"], slices[\"axi\"] = np.rot90(data[0, i]), np.rot90(data[0, :, j]), np.rot90(data[0, ..., k])\n",
    "\n",
    "            axes[row][col].imshow(slices[plane],**kwargs)\n",
    "            axes[row][col].axis('off')\n",
    "\n",
    "    if name is not None:\n",
    "        fig.suptitle(name)\n",
    "    plt.subplots_adjust( left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.05, hspace=0.05)\n",
    "            \n",
    "#     path = '../../outputs/'+folder\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "#     plt.savefig(path + str(name) + '.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_parameters(args,batch_size=8 ):\n",
    "    \"\"\"\n",
    "    Translate the names of the parameters between command line and source code.\n",
    "    \"\"\"\n",
    "    args.gpu = False\n",
    "    args.num_workers = args.nproc\n",
    "    args.optimizer = \"Adam\"\n",
    "    args.batch_size=batch_size\n",
    "    # args.loss = \"default\"\n",
    "\n",
    "    if hasattr(args, \"caps_dir\"):\n",
    "        args.input_dir = args.caps_dir\n",
    "    if hasattr(args, \"unnormalize\"):\n",
    "        args.minmaxnormalization = not args.unnormalize\n",
    "    if hasattr(args, \"slice_direction\"):\n",
    "        args.mri_plane = args.slice_direction\n",
    "    if hasattr(args, \"network_type\"):\n",
    "        args.mode_task = args.network_type\n",
    "\n",
    "    if not hasattr(args, \"selection_threshold\"):\n",
    "        args.selection_threshold = None\n",
    "        \n",
    "    if not hasattr(args, \"verbose\"):\n",
    "        args.verbose = 0\n",
    "    if not hasattr(args, \"bayesian\"):\n",
    "        args.bayesian = False\n",
    "\n",
    "    if not hasattr(args, \"prepare_dl\"):\n",
    "        if hasattr(args, \"use_extracted_features\"):\n",
    "            args.prepare_dl = args.use_extracted_features\n",
    "        elif hasattr(args, \"use_extracted_patches\") and args.mode == \"patch\":\n",
    "            args.prepare_dl = args.use_extracted_patches\n",
    "        elif hasattr(args, \"use_extracted_slices\") and args.mode == \"slice\":\n",
    "            args.prepare_dl = args.use_extracted_slices\n",
    "        elif hasattr(args, \"use_extracted_roi\") and args.mode == \"roi\":\n",
    "            args.prepare_dl = args.use_extracted_roi\n",
    "\n",
    "    return args\n",
    "\n",
    "def show_fpg_ax(img, axes, indices=None, plane=\"sag\"):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    print(img.shape)\n",
    "    if indices is None: \n",
    "        i, j, k = img.shape[0]//2, img.shape[1]//2, img.shape[2]//2\n",
    "    \n",
    "    kwargs = dict(cmap='gray', interpolation='none')\n",
    "    slices=dict()\n",
    "    slices[\"sag\"], slices[\"cor\"], slices[\"axi\"] = np.rot90(img[i]), np.rot90(img[:, j]), np.rot90(img[..., k])\n",
    "    axes.imshow(slices[plane],cmap='gray', interpolation='none')\n",
    "    axes.axis('off')\n",
    "\n",
    "    \n",
    "def get_unprocessed_images(participant_id, session_id, bids_directory=\"/u/horlavanasta/MasterProject/DataAndExperiments/Data/BIDS\"):\n",
    "    import numpy as np\n",
    "    import nibabel as nib\n",
    "    imgs_before=[]\n",
    "    for i in range(len(participant_id)):\n",
    "        image_path=os.path.join(bids_directory,  participant_id[i], session_id[i], \"anat\",participant_id[i] + '_' + session_id[i]\n",
    "                                   +'_T1w.nii.gz')\n",
    "        image_nii = nib.load(image_path)\n",
    "        image_np = image_nii.get_fdata()\n",
    "        imgs_before.append(image_np)\n",
    "    return imgs_before\n",
    "        \n",
    "    \n",
    "def show_preprocessing(imgs_before, imgs_after, num_cols=4, plane=\"sag\", MS=\"1.5T\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    fig, axes = plt.subplots(2, num_cols, \n",
    "                             figsize=((int(8 *num_cols), int(8 * 2)))\n",
    "                            )\n",
    "    for i in range(num_cols):\n",
    "        show_fpg_ax(imgs_before[i],axes[0][i] )\n",
    "        show_fpg_ax(imgs_after[i][0],axes[1][i])\n",
    "#     plt.subplots_adjust( left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "\n",
    "    plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01, wspace=0.02, hspace=0.02)\n",
    "    path = '../../plots'\n",
    "    plt.savefig(os.path.join(path, 'preprocessing_%s.png'%MS))\n",
    "    plt.close()\n",
    "    \n",
    "def show_data(model_folder, name=None, plane=\"sag\", show_augmentations=False, MS=\"1.5T\"):\n",
    "    from tools.deep_learning.models import init_model\n",
    "    from tools.deep_learning.data import (get_transforms,\n",
    "                                        load_data,\n",
    "                                        return_dataset,\n",
    "                                        generate_sampler)\n",
    "    from tools.deep_learning.iotools import return_logger\n",
    "    from argparse import Namespace\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch\n",
    "\n",
    "\n",
    "    path_params = os.path.join(model_folder, \"commandline_train.json\")\n",
    "    with open(path_params, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "    params = translate_parameters(Namespace(**params))\n",
    "    main_logger = return_logger(params.verbose, \"main process\")\n",
    "\n",
    "    \n",
    "    train_transforms, all_transforms = get_transforms(params.mode,\n",
    "                                                      minmaxnormalization=params.minmaxnormalization,\n",
    "                                                      data_augmentation=None,\n",
    "                                                      output_dir=None)\n",
    "    training_df, valid_df = load_data(\n",
    "            params.tsv_path,\n",
    "            params.diagnoses,\n",
    "            0,\n",
    "            n_splits=params.n_splits,\n",
    "            baseline=params.baseline,\n",
    "            logger=main_logger\n",
    "        )\n",
    "\n",
    "    \n",
    "    data_valid = return_dataset(params.mode, params.input_dir, valid_df, params.preprocessing,\n",
    "                                train_transformations=train_transforms, all_transformations=all_transforms,\n",
    "                                params=params)\n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        data_valid,\n",
    "        batch_size=params.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=params.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    sample = next(iter(valid_loader))\n",
    "    sample = next(iter(valid_loader))\n",
    "#     show_fpg(sample[\"image\"].numpy(), name=name, plane=plane)\n",
    "    participant_id, session_id =sample['participant_id'],sample[\"session_id\"]\n",
    "    imgs_after=sample[\"image\"].numpy()\n",
    "    imgs_before=get_unprocessed_images(participant_id, session_id)\n",
    "#     print(imgs_after[0].shape)\n",
    "    show_preprocessing(imgs_before, imgs_after, num_cols=4, plane=\"sag\", MS=MS)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS 3T \n",
      " ____________________________________________________________________________________________\n",
      "/u/horlavanasta/MasterProject/DataAndExperiments/Experiments_5-fold/Experiments-3T/NNs/ResNet18/subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse_20210612_195440\n",
      "(196, 256, 256)\n",
      "(169, 208, 179)\n",
      "(176, 240, 256)\n",
      "(169, 208, 179)\n",
      "(196, 256, 256)\n",
      "(169, 208, 179)\n",
      "(196, 256, 256)\n",
      "(169, 208, 179)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "folders = []\n",
    "MS_main_list = [\"3T\",]\n",
    "MS_list_dict = {'1.5T':['1.5T', '3T'], \"3T\": ['3T', '1.5T'], \"1.5T-3T\": [\"1.5T-3T\"]}\n",
    "home_folder='/u/horlavanasta/MasterProject/'\n",
    "\n",
    "isBayesian=True\n",
    "for MS in MS_main_list[:]:\n",
    "    print(\"MS %s \\n ____________________________________________________________________________________________\"%MS)\n",
    "    model_types = [\"ResNet18\"]\n",
    "    \n",
    "    model_dir_general = os.path.join(home_folder,\"DataAndExperiments/Experiments_5-fold/Experiments-\" + MS, \"NNs\" if isBayesian else \"NNs\")\n",
    "    for network in model_types[:]:\n",
    "        model_dir = os.path.join(model_dir_general, network)\n",
    "        # output_dir = pathlib.Path(output_dir)\n",
    "        modelPatter = \"subject_model*\"\n",
    "        folders = [f for f in pathlib.Path(model_dir).glob(modelPatter)]\n",
    "\n",
    "        for f in folders[:1]:\n",
    "            \n",
    "            print(f)\n",
    "#             show_model(f)\n",
    "            show_data(f, name=None, plane=\"sag\", show_augmentations=False, MS=MS)\n",
    "#             show_data(f, plane=\"sag\")\n",
    "#             show_data(f, plane=\"cor\")\n",
    "#             show_data(f, plane=\"axi\")\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
