{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py train /u/horlavanasta/MasterProject//DataAndExperiments/Experiments/Experiments-1.5T-3T/NNs_Bayesian/ResNet18/subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue --n_splits 1 --split 0  --batch_size 5               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_modes=[\"loss\", \"loss\",\"balanced_accuracy\"]\n",
    "selection_metrics=[\"best_loss\", \"last_checkpoint\", \"best_balanced_accuracy\"]\n",
    "for bayesian in [True, False]:\n",
    "# bayesian=True\n",
    "    for i in range(len(selection_metrics)):\n",
    "        history_mode=history_modes[i]\n",
    "        selection_metric=selection_metrics[i]\n",
    "        !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/plot_combined.py --history_mode $history_mode --selection_metric $selection_metric --bayesian $bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_history(model_path, num_folds):\n",
    "    from visualize.data_utils import get_data_generic\n",
    "    return os.path.exists(os.path.join(model_path, \"status.txt\"))\n",
    "    \n",
    "def check_results(model_path, MS_list, num_folds):\n",
    "    import os\n",
    "    import pathlib\n",
    "    import numpy as np\n",
    "    currentDirectory = pathlib.Path(model_path)\n",
    "    currentPattern = \"fold-*\"\n",
    "    flag=True\n",
    "    for fold_dir in currentDirectory.glob(currentPattern):\n",
    "        fold = int(str(fold_dir).split(\"-\")[-1])\n",
    "\n",
    "        selection_metrics = [\"best_loss\", \"best_balanced_accuracy\", \"last_checkpoint\"]\n",
    "        cnn_classification_dir = os.path.join(model_path, 'fold-%i' % fold, 'cnn_classification')\n",
    "        \n",
    "        for selection_metric in selection_metrics:\n",
    "            modes = ['train', 'validation']\n",
    "            for ms_el in MS_list:\n",
    "                modes.append('test_' + ms_el)\n",
    "                \n",
    "            for mode in modes:\n",
    "                if not os.path.exists(os.path.join(cnn_classification_dir, selection_metric,\n",
    "                                                       '%s_image_level_metrics.tsv' % (mode))):\n",
    "                    flag=False\n",
    "                \n",
    "    return flag\n",
    "    \n",
    "def check_complete_test(model_path, num_folds, MS_list):\n",
    "    import json\n",
    "    path_params = os.path.join(model_path, \"commandline_train.json\")\n",
    "    return (check_history(model_path, num_folds) and check_results(model_path, MS_list, num_folds))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_baesian_stat(model_path, MS_list, num_folds):\n",
    "    import os\n",
    "    import pathlib\n",
    "    import numpy as np\n",
    "    currentDirectory = pathlib.Path(model_path)\n",
    "    currentPattern = \"fold-*\"\n",
    "    flag=True\n",
    "    for fold_dir in currentDirectory.glob(currentPattern):\n",
    "        fold = int(str(fold_dir).split(\"-\")[-1])\n",
    "\n",
    "        selection_metrics = [\"best_loss\", \"best_balanced_accuracy\", \"last_checkpoint\"]\n",
    "        cnn_classification_dir = os.path.join(model_path, 'fold-%i' % fold, 'cnn_classification')\n",
    "        \n",
    "        for selection_metric in selection_metrics:\n",
    "            modes = ['test_' + ms_el for ms_el in MS_list]\n",
    "                \n",
    "            for mode in modes:\n",
    "                if not os.path.exists(os.path.join(cnn_classification_dir, selection_metric,\n",
    "                                                       '%s_image_level_stats.tsv' % (mode))):\n",
    "                    flag=False\n",
    "                \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_files={\n",
    "    True:{\n",
    "    \"1.5T\": {\n",
    "        True:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182808\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182814\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182826\"\n",
    "    ], \n",
    "        False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\"\n",
    "     ]\n",
    "    },\n",
    "    \"1.5T-3T\":{\n",
    "        True:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182745\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue-2\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182751\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\"\n",
    "    ], \n",
    "    False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\"\n",
    "     ]\n",
    "    },\n",
    "    \"3T\":{\n",
    "        True:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue-2\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue_20210626_182832\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue_20210626_182835\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue-2\"\n",
    "    ], \n",
    "    False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse_20210614_152919\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\"\n",
    "     ]\n",
    "    }   \n",
    "    \n",
    "    }, \n",
    "    \n",
    "    False:{\n",
    "    \"1.5T\":\n",
    "        {\n",
    "        True:{\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue-2\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\"\n",
    "        }, \n",
    "        False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse_20210612_195405\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\"\n",
    "     ]\n",
    "    }, \n",
    "    \"1.5T-3T\":\n",
    "        {\n",
    "        True:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue-2\"\n",
    "    ], \n",
    "        False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\"\n",
    "     ]\n",
    "    }, \n",
    "    \"3T\":\n",
    "        {\n",
    "        True:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue-2\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmTrue\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmTrue_20210612_195511\"\n",
    "    ], \n",
    "        False:[\n",
    "        \"subject_model-Conv5_FC3_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse_20210612_195526\", \n",
    "        \"subject_model-ResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-ResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-default_augmFalse\", \n",
    "        \"subject_model-SEResNet18_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\", \n",
    "        \"subject_model-SEResNet18Expanded_preprocessing-linear_task-AD_CN_norm-1_loss-WeightedCrossEntropy_augmFalse\"\n",
    "     ]\n",
    "    }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_vals(MS, MS_list,network, data_types, isBayesian):\n",
    "    \n",
    "    import argparse\n",
    "    from copy import deepcopy\n",
    "    import json\n",
    "    from visualize.data_utils import get_data_generic\n",
    "    printed_arr=[]\n",
    "    \n",
    "    home_folder='/u/horlavanasta/MasterProject/'  \n",
    "    data_types=[\"results\"]\n",
    "    model_dir_general = os.path.join(home_folder,\"DataAndExperiments/Experiments_5-fold/Experiments-\" + MS, \"NNs_Bayesian\" if isBayesian else \"NNs\", network)\n",
    "    \n",
    "    \n",
    "    args=argparse.Namespace()\n",
    "    loss_dict={\"default\":\"CE\", \"WeightedCrossEntropy\":\"WCE\"}\n",
    "    lr_dict={0.001:\"$10^{-3}$\", 0.0001:\"$10^{-4}$\", 1e-05:\"$10^{-5}$\"}\n",
    "\n",
    "    args.data_types=data_types\n",
    "    args.ba_inference_mode = \"mean\"\n",
    "    args.aggregation_type=\"average\"\n",
    "    args.MS_list = MS_list\n",
    "    args.output_path = None\n",
    "    args.bayesian = isBayesian\n",
    "    args.merged_file = os.path.join(home_folder, \"DataAndExperiments/Data/DataStat/merge.tsv\")\n",
    "    args.result_metrics = [\"sensitivity\", \"precision\", \"accuracy\", \"f1-score\"]\n",
    "    columns = deepcopy(args.result_metrics)\n",
    "    columns.append(\"mode\")\n",
    "    args.uncertainty_metric = \"total_variance\"\n",
    "    args.separate_by_MS = True if MS==\"1.5T-3T\" else False\n",
    "    args.selection_metrics=[\"best_balanced_accuracy\", \"best_loss\", \"last_checkpoint\"]\n",
    "    col_name={\"best_balanced_accuracy\": \"BBA\", \"best_loss\": \"BL\", \"last_checkpoint\":\"LE\"}\n",
    "    \n",
    "    \n",
    "    for augm in [False, True]:\n",
    "        \n",
    "        f=needed_files[isBayesian][MS][augm]\n",
    "        f=[s for s in f if \"-%s_\"%network in s][0]\n",
    "                           \n",
    "        filePath=os.path.join(model_dir_general, f)\n",
    "        path_params = os.path.join(filePath, \"commandline_train.json\")\n",
    "\n",
    "        with open(path_params, \"r\") as file_params:\n",
    "            params = json.load(file_params)\n",
    "    \n",
    "        printed_arr.append(\"& \\multirow{6}{*}{%s} & \\multirow{6}{*}{%s} & \\multirow{6}{*}{%s}\"%(augm, loss_dict[params[\"loss\"]], lr_dict[params[\"learning_rate\"]]))\n",
    "\n",
    "        args.model_path=filePath\n",
    "        \n",
    "        data = get_data_generic(args, MS)\n",
    "        data=data[\"average\"][\"results\"]\n",
    "        MS_list_printed=MS_list if not args.separate_by_MS else [\"1.5T\", \"3T\"]\n",
    "        printed_str=''\n",
    "        count=0\n",
    "        for i, selection_metric in enumerate(args.selection_metrics):\n",
    "            printed_arr.append(\"& \\multirow{2}{*}{%s} & \"%col_name[selection_metric])\n",
    "    #         print(\"& \\multirow{2}{*}{%s} & \"%col_name[selection_metric])\n",
    "            for j, MS_el in enumerate(MS_list_printed):\n",
    "\n",
    "                tmp2 = data[selection_metric].loc[data[selection_metric][\"mode\"] == \"test_%s\"%MS_el][args.result_metrics]\n",
    "                printed_str=MS_el\n",
    "                for res_metr in args.result_metrics:\n",
    "                    printed_str=printed_str+' & '+\"%.1f\"%(tmp2[res_metr].values[0]*100)\n",
    "\n",
    "                printed_arr.append(printed_str)\n",
    "    #             print(printed_str)\n",
    "                if j==0:\n",
    "\n",
    "                    el=\"\\\\\\ \\cline{6-10} &  &  &  &  & \"\n",
    "                elif count==5:\n",
    "                    el=\" \\\\\\ \\cline{2-10} \"\n",
    "    #             elif count==10:\n",
    "    #                 print(\"\\\\\\ \\hline\")\n",
    "                else:\n",
    "                    el=\"\\\\\\ \\cline{5-10} &  &  &\"\n",
    "                printed_arr.append(el)\n",
    "\n",
    "                count+=1\n",
    "                \n",
    "    printed_arr[-1]=\"\\\\\\ \\hline\"\n",
    "    for el in printed_arr:\n",
    "        print(el)\n",
    "    return printed_arr\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "True\n",
      " \n",
      "===========================================\n",
      "\n",
      "_______________________________________\n",
      "1.5T-3T\n",
      "______________________________\n",
      "\n",
      "_______________________________________\n",
      "SEResNet18Expanded\n",
      "______________________________\n",
      "\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 40.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/u/horlavanasta/.local/lib/python3.7/site-packages/seaborn/categorical.py:1296: UserWarning: 20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "MS_main_list = [ \"1.5T-3T\"]\n",
    "# MS_main_list = [\"1.5T-3T\"]\n",
    "\n",
    "num_folds=5\n",
    "MS_list_dict = {'1.5T':['1.5T', '3T'], \"3T\": ['3T', '1.5T'], \"1.5T-3T\": [\"1.5T-3T\"]}\n",
    "home_folder='/u/horlavanasta/MasterProject/'    \n",
    "merged_file=os.path.join(home_folder,\"DataAndExperiments/Data/DataStat\", \"merge.tsv\")\n",
    "# inference_modes=[\"mode\", \"mean\"]\n",
    "inference_modes=[\"mean\"]\n",
    "merged_file=os.path.join(home_folder,\"DataAndExperiments/Data/DataStat\", \"merge.tsv\")\n",
    "\n",
    "\n",
    "preprocessing=\"linear\"\n",
    "\n",
    "\n",
    "home_folder='/u/horlavanasta/MasterProject/'  \n",
    "    \n",
    "network_name={\"Conv5_FC3\":\"a\", \"ResNet18\":\"b\", \"ResNet18Expanded\":\"c\",\"SEResNet18\":\"d\", \"SEResNet18Expanded\":\"e\"}\n",
    "\n",
    "history_modes=[\"loss\", \"loss\",\"balanced_accuracy\"]\n",
    "selection_metrics=[\"best_loss\", \"last_checkpoint\", \"best_balanced_accuracy\"]\n",
    "num_folds=5\n",
    "\n",
    "for isBayesian in [True]:\n",
    "    print(\"===========================================\\n%s\\n \\n===========================================\\n\"%isBayesian)\n",
    "    for MS in MS_main_list[:]:\n",
    "        print(\"_______________________________________\\n%s\\n______________________________\\n\"%MS)\n",
    "\n",
    "        MS_list=MS_list_dict[MS]\n",
    "\n",
    "        results_folder_general =os.path.join(home_folder, 'Code/ClinicaTools/AD-DL/results/', \"Experiments_%d-fold\"%num_folds,\n",
    "                                             \"Experiments_Bayesian\" if isBayesian else \"Experiments\", 'Experiments-' + MS)\n",
    "\n",
    "        \n",
    "        \n",
    "        for network in [\"SEResNet18Expanded\"][:]:\n",
    "#         for network in [ \"ResNet18\", \"SEResNet18\"][:]:\n",
    "\n",
    "#             print(\"\\multirow{12}{*}{(%s)} \"%network_name[network])\n",
    "            print(\"_______________________________________\\n%s\\n______________________________\\n\"%network)\n",
    "            model_dir_general = os.path.join(home_folder,\"DataAndExperiments/Experiments_%d-fold/Experiments-%s\"%(num_folds, MS),\n",
    "                                         \"NNs_Bayesian\" if isBayesian else \"NNs\", network)\n",
    "            for augm in [False, True]:\n",
    "\n",
    "#                 print(\"& \\multirow{6}{*}{%s} & \\multirow{6}{*}{} & \\multirow{6}{*}{}\"%augm)\n",
    "                f=needed_files[isBayesian][MS][augm]\n",
    "                f=[s for s in f if \"-%s_\"%network in s][0]\n",
    "                f=os.path.join(model_dir_general, f)\n",
    "                \n",
    "                for i in range(len(selection_metrics)):\n",
    "#                     history_mode=history_modes[i]\n",
    "                    selection_metric=selection_metrics[i]\n",
    "                    results_dir=os.path.join(results_folder_general, \"preprocessing-%s\"%preprocessing, \"%s_selection\"%selection_metric)\n",
    "        \n",
    "\n",
    "                    if isBayesian:\n",
    "                        if not check_baesian_stat(f, num_folds=num_folds, MS_list=MS_list):\n",
    "                            prefixes = [\"test_\" + magnet_strength for magnet_strength in MS_list]\n",
    "                            !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py bayesian $f stat\n",
    "\n",
    "                        inference_mode=\"mean\"\n",
    "                        results_dir=os.path.join(results_dir, \"%s_inference\"%inference_mode)\n",
    "                        !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file --ba_inference_mode $inference_mode --history_modes \"loss\" \"balanced_accuracy\" --selection_metrics $selection_metric --uncertainty_metric \"total_variance\"\n",
    "                        !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types uncertainty_distribution --merged_file $merged_file --ba_inference_mode $inference_mode --selection_metrics $selection_metric --uncertainty_metric \"total_variance\" --aggregation_type \"separate\"\n",
    "\n",
    "#                         !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history uncertainty_distribution --merged_file $merged_file --ba_inference_mode $inference_mode\n",
    "                    else:\n",
    "                        !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file --history_modes \"loss\" \"balanced_accuracy\" --selection_metrics $selection_metric\n",
    "#                             !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "MS_main_list = ['1.5T',\"1.5T-3T\", '3T']\n",
    "num_folds=5\n",
    "MS_list_dict = {'1.5T':['1.5T', '3T'], \"3T\": ['3T', '1.5T'], \"1.5T-3T\": [\"1.5T-3T\"]}\n",
    "home_folder='/u/horlavanasta/MasterProject/'    \n",
    "merged_file=os.path.join(home_folder,\"DataAndExperiments/Data/DataStat\", \"merge.tsv\")\n",
    "# inference_modes=[\"mode\", \"mean\"]\n",
    "inference_modes=[\"mean\"]\n",
    "\n",
    "\n",
    "preprocessing=\"linear\"\n",
    "\n",
    "\n",
    "network_name={\"Conv5_FC3\":\"a\", \"ResNet18\":\"b\", \"ResNet18Expanded\":\"c\",\"SEResNet18\":\"d\", \"SEResNet18Expanded\":\"e\",  }\n",
    "\n",
    "for isBayesian in [True, False][:]:\n",
    "    print(\"===========================================\\n%s\\n \\n===========================================\\n\"%isBayesian)\n",
    "    for MS in MS_main_list[:]:\n",
    "        print(\"_______________________________________\\n%s\\n______________________________\\n\"%MS)\n",
    "\n",
    "        MS_list=MS_list_dict[MS]\n",
    "        \n",
    "        for network in [ \"Conv5_FC3\", \"ResNet18\", \"ResNet18Expanded\", \"SEResNet18\", \"SEResNet18Expanded\"][:]:\n",
    "            print(\"\\multirow{12}{*}{(%s)} \"%network_name[network])\n",
    "#             print(\"_______________________________________\\n%s\\n______________________________\\n\"%network)\n",
    "            \n",
    "#             for augm in [False, True]:\n",
    "#                 print(\"& \\multirow{6}{*}{%s} & \\multirow{6}{*}{} & \\multirow{6}{*}{}\"%augm)\n",
    "#                 f=needed_files[isBayesian][MS][augm]\n",
    "#                 f=[s for s in f if network in s][0]\n",
    "            printed_arr=print_table_vals(MS, MS_list, network, data_types=[\"results\"], isBayesian=isBayesian)\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "MS_main_list = ['1.5T', \"1.5T-3T\"]\n",
    "num_folds=5\n",
    "MS_list_dict = {'1.5T':['1.5T', '3T'], \"3T\": ['3T', '1.5T'], \"1.5T-3T\": [\"1.5T-3T\"]}\n",
    "home_folder='/u/horlavanasta/MasterProject/'    \n",
    "merged_file=os.path.join(home_folder,\"DataAndExperiments/Data/DataStat\", \"merge.tsv\")\n",
    "# inference_modes=[\"mode\", \"mean\"]\n",
    "inference_modes=[\"mean\"]\n",
    "\n",
    "\n",
    "\n",
    "preprocessing=\"linear\"\n",
    "history_modes=[\"loss\", \"loss\",\"balanced_accuracy\"]\n",
    "selection_metrics=[\"best_loss\", \"last_checkpoint\", \"best_balanced_accuracy\"]\n",
    "isBayesian=True\n",
    "for isBayesian in [True]:\n",
    "    for MS in MS_main_list[:]:\n",
    "        print(\"MS %s \\n ____________________________________________________________________________________________\"%MS)\n",
    "        model_types = [ \"Conv5_FC3\", \"ResNet18\", \"SEResNet18\", \"ResNet18Expanded\", \"SEResNet18Expanded\"]\n",
    "        MS_list = MS_list_dict[MS]\n",
    "\n",
    "        results_folder_general =os.path.join(home_folder, 'Code/ClinicaTools/AD-DL/results/', \"Experiments_%d-fold\"%num_folds,\n",
    "                                             \"Experiments_Bayesian\" if isBayesian else \"Experiments\", 'Experiments-' + MS)\n",
    "\n",
    "        model_dir_general = os.path.join(home_folder,\"DataAndExperiments/Experiments_%d-fold/Experiments-%s\"%(num_folds, MS),\n",
    "                                         \"NNs_Bayesian\" if isBayesian else \"NNs\")\n",
    "    #         print(args.preprocessing)\n",
    "\n",
    "        for network in model_types[:]:\n",
    "\n",
    "            model_dir = os.path.join(model_dir_general, network)\n",
    "            modelPatter = \"subject_model*\"\n",
    "            folders = [f for f in pathlib.Path(model_dir).glob(modelPatter)]\n",
    "            models_list=[]\n",
    "\n",
    "            for f in folders[:]:\n",
    "                for i in range(len(selection_metrics)):\n",
    "#                     history_mode=history_modes[i]\n",
    "                    selection_metric=selection_metrics[i]\n",
    "                    results_dir=os.path.join(results_folder_general, \"preprocessing-%s\"%preprocessing, \"%s_selection\"%selection_metric)\n",
    "    #                 results_dir=os.path.join(results_folder_general, \"preprocessing-%s\"%preprocessing)\n",
    "\n",
    "    #                 print(results_dir)\n",
    "                    if check_complete_test(f, num_folds=num_folds, MS_list=MS_list) and \"_preprocessing-%s_\"%preprocessing in str(f):\n",
    "                        if isBayesian:\n",
    "#                             if not check_baesian_stat(f, num_folds=num_folds, MS_list=MS_list):\n",
    "#                                 prefixes = [\"test_\" + magnet_strength for magnet_strength in MS_list]\n",
    "#                                 !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py bayesian $f stat\n",
    "                          \n",
    "                            for inference_mode in inference_modes:\n",
    "                                results_dir=os.path.join(results_dir, \"%s_inference\"%inference_mode)\n",
    "#                                 !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file --ba_inference_mode $inference_mode --history_modes \"loss\" \"balanced_accuracy\" --selection_metrics $selection_metric --uncertainty_metric \"total_variance\"\n",
    "                                !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types uncertainty_distribution --merged_file $merged_file --ba_inference_mode $inference_mode --selection_metrics $selection_metric --uncertainty_metric \"total_variance\" --aggregation_type \"separate\"\n",
    "\n",
    "    #                             !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history uncertainty_distribution --merged_file $merged_file --ba_inference_mode $inference_mode\n",
    "                        else:\n",
    "                                !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file --history_modes \"loss\" \"balanced_accuracy\" --selection_metrics $selection_metric\n",
    "    #                             !python ~/MasterProject/Code/ClinicaTools/AD-DL/clinicaaddl/clinicaaddl/main.py visualize --model_path $f --output_path $results_dir --data_types results history --merged_file $merged_file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_parameters(args):\n",
    "    \"\"\"\n",
    "    Translate the names of the parameters between command line and source code.\n",
    "    \"\"\"\n",
    "    args.gpu = False\n",
    "    args.num_workers = args.nproc\n",
    "    args.optimizer = \"Adam\"\n",
    "    args.batch_size=9\n",
    "    # args.loss = \"default\"\n",
    "\n",
    "    if hasattr(args, \"caps_dir\"):\n",
    "        args.input_dir = args.caps_dir\n",
    "    if hasattr(args, \"unnormalize\"):\n",
    "        args.minmaxnormalization = not args.unnormalize\n",
    "    if hasattr(args, \"slice_direction\"):\n",
    "        args.mri_plane = args.slice_direction\n",
    "    if hasattr(args, \"network_type\"):\n",
    "        args.mode_task = args.network_type\n",
    "\n",
    "    if not hasattr(args, \"selection_threshold\"):\n",
    "        args.selection_threshold = None\n",
    "        \n",
    "    if not hasattr(args, \"verbose\"):\n",
    "        args.verbose = 0\n",
    "    if not hasattr(args, \"bayesian\"):\n",
    "        args.bayesian = False\n",
    "\n",
    "    if not hasattr(args, \"prepare_dl\"):\n",
    "        if hasattr(args, \"use_extracted_features\"):\n",
    "            args.prepare_dl = args.use_extracted_features\n",
    "        elif hasattr(args, \"use_extracted_patches\") and args.mode == \"patch\":\n",
    "            args.prepare_dl = args.use_extracted_patches\n",
    "        elif hasattr(args, \"use_extracted_slices\") and args.mode == \"slice\":\n",
    "            args.prepare_dl = args.use_extracted_slices\n",
    "        elif hasattr(args, \"use_extracted_roi\") and args.mode == \"roi\":\n",
    "            args.prepare_dl = args.use_extracted_roi\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def show_fpg(data_batch, indices=None,plane=\"sag\", num_rows=2, \n",
    "    num_cols=2, name=None, folder=\"/current_augmentations_examples/\"):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=((int(8 * num_rows), int(6 * num_cols))))\n",
    "    data_batch=data_batch.numpy()\n",
    "    print(data_batch.shape)\n",
    "    data_batch=data_batch[:num_rows*num_cols].reshape(num_rows, num_cols, data_batch.shape[1],  data_batch.shape[2],  data_batch.shape[3], \n",
    "                                  data_batch.shape[4])\n",
    "    print(data_batch.shape)\n",
    "\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_cols):\n",
    "\n",
    "            i, j, k = indices\n",
    "            data=data_batch[row][col]\n",
    "            kwargs = dict(cmap='gray', interpolation='none')\n",
    "            slices=dict()\n",
    "            slices[\"sag\"], slices[\"cor\"], slices[\"axi\"] = np.rot90(data[0, i]), np.rot90(data[0, :, j]), np.rot90(data[0, ..., k])\n",
    "\n",
    "            axes[row][col].imshow(slices[plane],**kwargs)\n",
    "            axes[row][col].axis('off')\n",
    "            \n",
    "#     path = '../../outputs/'+folder\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "    if name is not None:\n",
    "        fig.suptitle(name)\n",
    "    plt.subplots_adjust( left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.05, hspace=0.05)\n",
    "\n",
    "#     plt.savefig(path + str(name) + '.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def show_data(model_folder, name=None, plane=\"sag\"):\n",
    "    from tools.deep_learning.models import init_model\n",
    "    from tools.deep_learning.data import (get_transforms,\n",
    "                                        load_data,\n",
    "                                        return_dataset,\n",
    "                                        generate_sampler)\n",
    "    from tools.deep_learning.iotools import return_logger\n",
    "    from argparse import Namespace\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    import torchvision.models\n",
    "    import hiddenlayer as hl\n",
    "    import torch\n",
    "\n",
    "    path_params = os.path.join(model_folder, \"commandline_train.json\")\n",
    "    with open(path_params, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "    params = translate_parameters(Namespace(**params))\n",
    "    main_logger = return_logger(params.verbose, \"main process\")\n",
    "\n",
    "    \n",
    "    train_transforms, all_transforms = get_transforms(params.mode,\n",
    "                                                      minmaxnormalization=params.minmaxnormalization,\n",
    "                                                      data_augmentation=None,\n",
    "                                                      output_dir=None)\n",
    "    training_df, valid_df = load_data(\n",
    "            params.tsv_path,\n",
    "            params.diagnoses,\n",
    "            0,\n",
    "            n_splits=params.n_splits,\n",
    "            baseline=params.baseline,\n",
    "            logger=main_logger\n",
    "        )\n",
    "\n",
    "    \n",
    "    data_valid = return_dataset(params.mode, params.input_dir, valid_df, params.preprocessing,\n",
    "                                train_transformations=train_transforms, all_transformations=all_transforms,\n",
    "                                params=params)\n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        data_valid,\n",
    "        batch_size=params.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=params.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    sample = next(iter(valid_loader))\n",
    "    \n",
    "    show_fpg(sample[\"image\"], indices=(169//2, 208//2, 179//2), name=name, plane=plane)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "folders = []\n",
    "MS_main_list = ['1.5T', \"3T\",\"1.5T-3T\" ]\n",
    "MS_list_dict = {'1.5T':['1.5T', '3T'], \"3T\": ['3T', '1.5T'], \"1.5T-3T\": [\"1.5T-3T\"]}\n",
    "home_folder='/u/horlavanasta/MasterProject/'\n",
    "\n",
    "isBayesian=True\n",
    "for MS in MS_main_list[:1]:\n",
    "    print(\"MS %s \\n ____________________________________________________________________________________________\"%MS)\n",
    "    model_types = [ \"ResNet18\", \"SEResNet18\", \"ResNet18Expanded\", \"SEResNet18Expanded\", \"Conv5_FC3\" ]\n",
    "    MS_list = MS_list_dict[MS]\n",
    "    \n",
    "    model_dir_general = os.path.join(home_folder,\"DataAndExperiments/Experiments/Experiments-\" + MS, \"NNs\" if isBayesian else \"NNs\")\n",
    "\n",
    "    for network in model_types[:]:\n",
    "        model_dir = os.path.join(model_dir_general, network)\n",
    "        # output_dir = pathlib.Path(output_dir)\n",
    "        modelPatter = \"subject_model*\"\n",
    "        folders = [f for f in pathlib.Path(model_dir).glob(modelPatter)]\n",
    "\n",
    "        for f in folders[:1]:\n",
    "            print(f)\n",
    "            show_model(f)\n",
    "#             show_data(f, plane=\"sag\")\n",
    "#             show_data(f, plane=\"cor\")\n",
    "#             show_data(f, plane=\"axi\")\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
